{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "review_list = []\n",
    "\n",
    "def analyze_review(row,review_list):\n",
    "    if str(row['sentiment']).strip().lower() in ['positive','negative']:\n",
    "        review_list.append({\"review\":row[\"review\"],\"sentiment\":row[\"sentiment\"]})\n",
    "        return None\n",
    "    \n",
    "    review = row['sentiment'] + row['review']\n",
    "    sentiment,i = \"\",2\n",
    "    while row['Unnamed: '+str(i)].strip().lower() not in ['positive','negative']:\n",
    "        review += row['Unnamed: '+str(i)]\n",
    "        i+=1\n",
    "        \n",
    "    sentiment = row['Unnamed: '+str(i)]\n",
    "    review_list.append({\"review\":review,\"sentiment\":sentiment})\n",
    "\n",
    "df.apply(analyze_review,review_list=review_list,axis=1)\n",
    "df_processed = pd.DataFrame.from_dict(review_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding the binary target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "training_reviews,testing_reviews,training_labels,testing_labels = train_test_split(df_processed['review'].values,df_processed['sentiment'].values,test_size=0.2)\n",
    "training_labels = le.fit_transform(training_labels)\n",
    "testing_labels = le.fit_transform(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the sentences and converting them to sequences with max len 200 (200 dimensional vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_reviews)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequence = tokenizer.texts_to_sequences(training_reviews)\n",
    "testing_sequence = tokenizer.texts_to_sequences(testing_reviews)\n",
    "train_pad_sequence = pad_sequences(training_sequence,maxlen=200,truncating='post',padding='pre') #truncating 'post' means removing words to fit the maxlen from the end of the sentence\n",
    "test_pad_sequence = pad_sequences(testing_sequence,maxlen=200,truncating='post',padding='pre') #padding 'pre' means adding words in the beginning of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"0\": \"Negative\",\n",
    "    \"1\": \"Positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = b'{\\n    \"predictions\": [[0.808536172]\\n    ]\\n}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.808536172]]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could not agree less with the rating that was given to this movie, and I believe this is a sample of how short minded most of spectators are all over the world. Really... Are you forgetting that Cinema used to be a kind of art before some tycoons tried to make it only entertainment? This movie is not entertainment, at least not that easy entertainment you get on movies like Titanic or Gladiator. It has style, it is different, it is shocking... That's why most of you have hated it so much: because it does not try to be pleasing to you. It's just a story, a very weird one I admit, but after all, only a weird story. It is not a great story, not even a great cinema work, but I believe it is worth a 7-stars rating only for the courage of both author and director to shot a story that is not made to please the audience, thus selling billions of copies and making the big studios even richer. This movie is, for me, European-artistic-like movie made in the US, and everyone involved in the making of it deserves respect. Be it for the courage, or be it for the unique sense of humor.\n"
     ]
    }
   ],
   "source": [
    "index_no = 5\n",
    "print(training_reviews[index_no])\n",
    "sentence = training_reviews[index_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_conf = tokenizer.to_json()\n",
    "with open(\"tokenizer.json\",\"w\") as f:\n",
    "    json.dump(tokenizer_conf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer.json\",\"r\") as f:\n",
    "    tokenizer_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[int(round(model.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]),maxlen=200,truncating='post',padding='pre')).ravel()[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8085362], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pad_sequences(tokenizer.texts_to_sequences([sentence]),maxlen=200,truncating='post',padding='pre')).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   10,    98,    21,  1020,   341,    16,     1,   657,    12,\n",
       "           13,   353,     5,    11,    17,     2,    10,   259,    11,\n",
       "            6,     3,  9661,     4,    86,   338,  2846,    88,     4,\n",
       "        13135,    23,    29,   121,     1,   181,    62,    23,    22,\n",
       "         7540,    12,   436,   330,     5,    26,     3,   241,     4,\n",
       "          506,   159,    47, 27745,   766,     5,    93,     9,    61,\n",
       "          741,    11,    17,     6,    21,   741,    30,   222,    21,\n",
       "           12,   753,   741,    22,    74,    20,    97,    37,  3643,\n",
       "           38, 10235,     9,    45,   399,     9,     6,   278,     9,\n",
       "            6,  1621,   196,   134,    88,     4,    22,    25,  1733,\n",
       "            9,    34,    72,    84,     9,   124,    21,   365,     5,\n",
       "           26,  5374,     5,    22,    44,    39,     3,    63,     3,\n",
       "           52,   929,    27,    10,   994,    18,   100,    29,    61,\n",
       "            3,   929,    63,     9,     6,    21,     3,    78,    63,\n",
       "           21,    57,     3,    78,   436,   157,    18,    10,   259,\n",
       "            9,     6,   275,     3,   705,   406,   657,    61,    15,\n",
       "            1,  3019,     4,   195,  2099,     2,   164,     5,   319,\n",
       "            3,    63,    12,     6,    21,    90,     5,   600,     1,\n",
       "          306,  1386,  3259, 18712,     4,  4092,     2,   232,     1,\n",
       "          191,  2531,    57, 11145,    11,    17,     6,    15,    68,\n",
       "         2042,  1675,    37,    17,    90,     8,     1,   179,     2,\n",
       "          310,   574,     8,     1,   232,     4,     9,  1017,  1188,\n",
       "           26,     9]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(tokenizer.texts_to_sequences([sentence]),maxlen=200,truncating='post',padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the 200 dimensional GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using glove vectors for embedding\n",
    "embedded_words = {}\n",
    "with open(\"glove.6B.200d.txt\") as f:\n",
    "    for line in f:\n",
    "        words, coeff = line.split(maxsplit=1)\n",
    "        coeff = np.array(coeff.split(),dtype=float)\n",
    "        embedded_words[words] = coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, 200))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embedded_words.get(word)\n",
    "    if embedding_vector is not None: #if we cannot find the embedding vector from glove embedding, it will just be zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 200)         22467200  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         179328    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 22,777,985\n",
      "Trainable params: 310,785\n",
      "Non-trainable params: 22,467,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(None,),dtype='int64')\n",
    "x = layers.Embedding(len(word_index)+1,200,weights=[embedding_matrix],input_length=200,trainable=False)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "#Conv1D + global max pooling\n",
    "x = layers.Conv1D(128,7,padding='valid',activation='relu',strides=2)(x)\n",
    "x = layers.Conv1D(128,7,padding='valid',activation='relu',strides=2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "#Hidden layer\n",
    "x = layers.Dense(128,activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = layers.Dense(1,activation='sigmoid',name='predictions')(x)\n",
    "model = tf.keras.Model(inputs,predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model callbacks and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Already a folder with a similar version in the config exists. Kindly check before proceeding!! Once the training     starts, the data would be overrated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\",\"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "if os.path.isdir(os.path.join(MODEL_FOLDER,str(model_version))):\n",
    "    warnings.warn(\"Already a folder with a similar version in the config exists. Kindly check before proceeding!! Once the training starts, the data would be overrated\")\n",
    "\n",
    "model_version = model_config['training_version']\n",
    "MODEL_FOLDER = model_config['model_folder']\n",
    "export_path = os.path.join(MODEL_FOLDER,str(model_version))\n",
    "log_dir = os.path.join(export_path,f\"logs\")\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(export_path,\"checkpoints\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir, #the folder is created automatically if not present\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=5\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7243WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 0.5349 - accuracy: 0.7244 - val_loss: 0.4162 - val_accuracy: 0.8135 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.4386 - accuracy: 0.7978INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 0.4384 - accuracy: 0.7980 - val_loss: 0.3817 - val_accuracy: 0.8288 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8127INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.4095 - accuracy: 0.8127 - val_loss: 0.3732 - val_accuracy: 0.8371 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1245/1250 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8235INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.3917 - accuracy: 0.8235 - val_loss: 0.3685 - val_accuracy: 0.8336 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1246/1250 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8318INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 0.3761 - accuracy: 0.8317 - val_loss: 0.3604 - val_accuracy: 0.8369 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.3637 - accuracy: 0.8396 - val_loss: 0.3608 - val_accuracy: 0.8386 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 0.3521 - accuracy: 0.8443 - val_loss: 0.3675 - val_accuracy: 0.8381 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 0.3411 - accuracy: 0.8491 - val_loss: 0.3680 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.3270 - accuracy: 0.8586 - val_loss: 0.3912 - val_accuracy: 0.8341 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.3170 - accuracy: 0.8622 - val_loss: 0.3695 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.2811 - accuracy: 0.8808 - val_loss: 0.3698 - val_accuracy: 0.8447 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.2760 - accuracy: 0.8832 - val_loss: 0.3693 - val_accuracy: 0.8447 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "1249/1250 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.8847INFO:tensorflow:Assets written to: models/1/checkpoints/assets\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.2698 - accuracy: 0.8848 - val_loss: 0.3603 - val_accuracy: 0.8472 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.2676 - accuracy: 0.8848 - val_loss: 0.3624 - val_accuracy: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 42s 34ms/step - loss: 0.2606 - accuracy: 0.8894 - val_loss: 0.3752 - val_accuracy: 0.8422 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.2615 - accuracy: 0.8900 - val_loss: 0.3672 - val_accuracy: 0.8466 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 56s 45ms/step - loss: 0.2547 - accuracy: 0.8921 - val_loss: 0.3648 - val_accuracy: 0.8453 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = callbacks_list\n",
    "history = model.fit(train_pad_sequence,training_labels,epochs=50,validation_data=(test_pad_sequence,testing_labels),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model, model config and the tokenizer for future inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/1/assets\n"
     ]
    }
   ],
   "source": [
    "model_config['serving_version'] = model_version\n",
    "model_config['training_version'] = str(int(model_version) + 1)\n",
    "\n",
    "with open(\"config.json\",\"w\") as f:\n",
    "    json.dump(model_config,f)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    export_path,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        model.save(f\"{MODEL_FOLDER}/{model_version}/weight_file/model.h5\")\n",
    "    except OSError:\n",
    "        os.mkdir(f\"{MODEL_FOLDER}/{model_version}/weight_file\")\n",
    "        continue\n",
    "    break\n",
    "\n",
    "tokenizer_conf = tokenizer.to_json()\n",
    "with open(os.path.join(export_path,\"tokenizer.json\"),\"w\") as f:\n",
    "    json.dump(tokenizer_conf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2ede645abdcd0483\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2ede645abdcd0483\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logdir --host 0.0.0.0 #port 6006 for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/3/weight_file/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model,to_file=\"model_new.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pydotplus 2.0.2\n",
      "Uninstalling pydotplus-2.0.2:\n",
      "  Successfully uninstalled pydotplus-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pydot -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /root/.cache/pip/wheels/1f/5c/ba/f931f74fcac8f48b18ae597279203b1c1f92fc76249c2b6f66/pydotplus-2.0.2-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from pydotplus) (2.4.7)\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  graphviz libpython-stdlib libpython2.7-minimal libpython2.7-stdlib python\n",
      "  python-minimal python-pyparsing python2.7 python2.7-minimal\n",
      "Suggested packages:\n",
      "  gsfonts graphviz-doc python-doc python-tk python-pyparsing-doc python2.7-doc\n",
      "  binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  graphviz libpython-stdlib libpython2.7-minimal libpython2.7-stdlib python\n",
      "  python-minimal python-pydot python-pyparsing python2.7 python2.7-minimal\n",
      "0 upgraded, 10 newly installed, 0 to remove and 46 not upgraded.\n",
      "Need to get 4640 kB of archives.\n",
      "After this operation, 20.3 MB of additional disk space will be used.\n",
      "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-minimal amd64 2.7.17-1~18.04ubuntu1\n",
      "Ign:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7-minimal amd64 2.7.17-1~18.04ubuntu1\n",
      "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-minimal amd64 2.7.17-1~18.04ubuntu1\n",
      "  404  Not Found [IP: 91.189.91.39 80]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-minimal amd64 2.7.15~rc1-1 [28.1 kB]\n",
      "Err:2 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7-minimal amd64 2.7.17-1~18.04ubuntu1\n",
      "  404  Not Found [IP: 91.189.91.39 80]\n",
      "Ign:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-stdlib amd64 2.7.17-1~18.04ubuntu1\n",
      "Err:4 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libpython2.7-stdlib amd64 2.7.17-1~18.04ubuntu1\n",
      "  404  Not Found [IP: 91.189.91.38 80]\n",
      "Ign:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7 amd64 2.7.17-1~18.04ubuntu1\n",
      "Err:5 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 python2.7 amd64 2.7.17-1~18.04ubuntu1\n",
      "  404  Not Found [IP: 91.189.91.38 80]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpython-stdlib amd64 2.7.15~rc1-1 [7620 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 python amd64 2.7.15~rc1-1 [140 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 graphviz amd64 2.40.1-2 [601 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pyparsing all 2.2.0+dfsg1-2 [52.1 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-pydot all 1.2.3-1 [19.6 kB]\n",
      "Fetched 848 kB in 5s (162 kB/s)          \n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-minimal_2.7.17-1~18.04ubuntu1_amd64.deb  404  Not Found [IP: 91.189.91.39 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7-minimal_2.7.17-1~18.04ubuntu1_amd64.deb  404  Not Found [IP: 91.189.91.39 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-stdlib_2.7.17-1~18.04ubuntu1_amd64.deb  404  Not Found [IP: 91.189.91.38 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7_2.7.17-1~18.04ubuntu1_amd64.deb  404  Not Found [IP: 91.189.91.38 80]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
     ]
    }
   ],
   "source": [
    "!apt-get install python-pydot -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
